{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz-RRWaZwIE_"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOLh456Zj-vi"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.aws && cp /content/drive/MyDrive/AWS/684947_admin ~/.aws/credentials\n",
        "!chmod 600 ~/.aws/credentials\n",
        "!pip install -qq awscli boto3\n",
        "!aws sts get-caller-identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZf2xdtzAYWV"
      },
      "source": [
        "### Create Cloudformation Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDgPfcNWAmHG",
        "outputId": "28f9f32b-2793-431f-fefc-a1d90f89bac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"StackId\": \"arn:aws:cloudformation:us-east-1:684199068947:stack/RedshiftWorkshop/2940aec0-6c03-11ed-bb3f-0e972e325d17\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!aws cloudformation create-stack \\\n",
        "--stack-name RedshiftWorkshop \\\n",
        "--template-body file://redshift_workshop.yml \\\n",
        "--capabilities CAPABILITY_NAMED_IAM \\\n",
        "--parameters \\\n",
        "ParameterKey=EETeamRoleArn,ParameterValue=arn:aws:iam::684199068947:user/sparsh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bb4jsHQEGsa"
      },
      "source": [
        "### Configure Client Tool - Query Editor V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J72SLY0mEKUN"
      },
      "source": [
        "1. On the left-hand side, click on the Redshift cluster you want to connect to.\n",
        "1. A pop-up window should have opened.\n",
        "1. If your IAM User/Role has the privilege \"redshift:GetClusterCredentials\", you can use the Temporary credentials option.\n",
        "1. Enter the Database name `dev` and user name `awsuser`. Click connect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgEv-hATEwEm"
      },
      "source": [
        "### Run Sample Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTLeQc3CE1S3"
      },
      "source": [
        "Run the following query to list the users within the redshift cluster.\n",
        "\n",
        "```sql\n",
        "select * from pg_user\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIjcroovDQt4"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giW8_42ADVta"
      },
      "source": [
        "In this lab, you will use a set of eight tables based on the TPC Benchmark data model. You create these tables within your Redshift cluster then load these tables with sample data stored in S3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26SoNhEbDfpJ"
      },
      "source": [
        "### Create Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZz4JI7vDe51"
      },
      "source": [
        "Copy the following create table statements to create tables in the database mimicking the TPC Benchmark data model.\n",
        "\n",
        "```sql\n",
        "DROP TABLE IF EXISTS partsupp;\n",
        "DROP TABLE IF EXISTS lineitem;\n",
        "DROP TABLE IF EXISTS supplier;\n",
        "DROP TABLE IF EXISTS part;\n",
        "DROP TABLE IF EXISTS orders;\n",
        "DROP TABLE IF EXISTS customer;\n",
        "DROP TABLE IF EXISTS nation;\n",
        "DROP TABLE IF EXISTS region;\n",
        "\n",
        "CREATE TABLE region (\n",
        "  R_REGIONKEY bigint NOT NULL,\n",
        "  R_NAME varchar(25),\n",
        "  R_COMMENT varchar(152))\n",
        "diststyle all;\n",
        "\n",
        "CREATE TABLE nation (\n",
        "  N_NATIONKEY bigint NOT NULL,\n",
        "  N_NAME varchar(25),\n",
        "  N_REGIONKEY bigint,\n",
        "  N_COMMENT varchar(152))\n",
        "diststyle all;\n",
        "\n",
        "create table customer (\n",
        "  C_CUSTKEY bigint NOT NULL,\n",
        "  C_NAME varchar(25),\n",
        "  C_ADDRESS varchar(40),\n",
        "  C_NATIONKEY bigint,\n",
        "  C_PHONE varchar(15),\n",
        "  C_ACCTBAL decimal(18,4),\n",
        "  C_MKTSEGMENT varchar(10),\n",
        "  C_COMMENT varchar(117))\n",
        "diststyle all;\n",
        "\n",
        "create table orders (\n",
        "  O_ORDERKEY bigint NOT NULL,\n",
        "  O_CUSTKEY bigint,\n",
        "  O_ORDERSTATUS varchar(1),\n",
        "  O_TOTALPRICE decimal(18,4),\n",
        "  O_ORDERDATE Date,\n",
        "  O_ORDERPRIORITY varchar(15),\n",
        "  O_CLERK varchar(15),\n",
        "  O_SHIPPRIORITY Integer,\n",
        "  O_COMMENT varchar(79))\n",
        "distkey (O_ORDERKEY)\n",
        "sortkey (O_ORDERDATE);\n",
        "\n",
        "create table part (\n",
        "  P_PARTKEY bigint NOT NULL,\n",
        "  P_NAME varchar(55),\n",
        "  P_MFGR  varchar(25),\n",
        "  P_BRAND varchar(10),\n",
        "  P_TYPE varchar(25),\n",
        "  P_SIZE integer,\n",
        "  P_CONTAINER varchar(10),\n",
        "  P_RETAILPRICE decimal(18,4),\n",
        "  P_COMMENT varchar(23))\n",
        "diststyle all;\n",
        "\n",
        "create table supplier (\n",
        "  S_SUPPKEY bigint NOT NULL,\n",
        "  S_NAME varchar(25),\n",
        "  S_ADDRESS varchar(40),\n",
        "  S_NATIONKEY bigint,\n",
        "  S_PHONE varchar(15),\n",
        "  S_ACCTBAL decimal(18,4),\n",
        "  S_COMMENT varchar(101))\n",
        "diststyle all;                                                              \n",
        "\n",
        "create table lineitem (\n",
        "  L_ORDERKEY bigint NOT NULL,\n",
        "  L_PARTKEY bigint,\n",
        "  L_SUPPKEY bigint,\n",
        "  L_LINENUMBER integer NOT NULL,\n",
        "  L_QUANTITY decimal(18,4),\n",
        "  L_EXTENDEDPRICE decimal(18,4),\n",
        "  L_DISCOUNT decimal(18,4),\n",
        "  L_TAX decimal(18,4),\n",
        "  L_RETURNFLAG varchar(1),\n",
        "  L_LINESTATUS varchar(1),\n",
        "  L_SHIPDATE date,\n",
        "  L_COMMITDATE date,\n",
        "  L_RECEIPTDATE date,\n",
        "  L_SHIPINSTRUCT varchar(25),\n",
        "  L_SHIPMODE varchar(10),\n",
        "  L_COMMENT varchar(44))\n",
        "distkey (L_ORDERKEY)\n",
        "sortkey (L_RECEIPTDATE);\n",
        "\n",
        "create table partsupp (\n",
        "  PS_PARTKEY bigint NOT NULL,\n",
        "  PS_SUPPKEY bigint NOT NULL,\n",
        "  PS_AVAILQTY integer,\n",
        "  PS_SUPPLYCOST decimal(18,4),\n",
        "  PS_COMMENT varchar(199))\n",
        "diststyle even;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMzpaVX5NBIp"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsz3YqhM_1c"
      },
      "source": [
        "A COPY command loads large amounts of data much more efficiently than using INSERT statements, and stores the data more effectively as well. Use a single COPY command to load data for one table from multiple files. Amazon Redshift then automatically loads the data in parallel. For your convenience, the sample data you will use is available in a public Amazon S3 bucket. To ensure that Redshift performs a compression analysis, set the COMPUPDATE parameter to ON in your COPY commands.\n",
        "\n",
        "```sql\n",
        "COPY region FROM 's3://redshift-immersionday-labs/data/region/region.tbl.lzo'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "COPY nation FROM 's3://redshift-immersionday-labs/data/nation/nation.tbl.'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy customer from 's3://redshift-immersionday-labs/data/customer/customer.tbl.'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy orders from 's3://redshift-immersionday-labs/data/orders/orders.tbl.'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy part from 's3://redshift-immersionday-labs/data/part/part.tbl.'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy supplier from 's3://redshift-immersionday-labs/data/supplier/supplier.json' manifest\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy lineitem from 's3://redshift-immersionday-labs/data/lineitem-part/'\n",
        "iam_role default\n",
        "region 'us-west-2' gzip delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "copy partsupp from 's3://redshift-immersionday-labs/data/partsupp/partsupp.tbl.'\n",
        "iam_role default\n",
        "region 'us-west-2' lzop delimiter '|' COMPUPDATE PRESET;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xecAKqbyPwxK"
      },
      "source": [
        "The estimated time to load the data is as follows, note you can check timing information on actions in the performance and query tabs on the redshift console:\n",
        "\n",
        "```\n",
        "REGION (5 rows) - 20s\n",
        "NATION (25 rows) - 2s\n",
        "CUSTOMER (15M rows) – 31s\n",
        "ORDERS - (76M rows) - 13s\n",
        "PART - (20M rows) - 34s\n",
        "SUPPLIER - (1M rows) - 7s\n",
        "LINEITEM - (303M rows) - 48s\n",
        "PARTSUPPLIER - (80M rows) 12s\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Raw4lPtmQwes"
      },
      "source": [
        "A few key takeaways from the above COPY statements:\n",
        "\n",
        "- COMPUPDATE PRESET ON will assign compression using the Amazon Redshift best practices related to the data type of the column but without analyzing the data in the table.\n",
        "- COPY for the REGION table points to a specfic file (region.tbl.lzo) while COPY for other tables point to a prefix to multiple files (lineitem.tbl.)\n",
        "- COPY for the SUPPLIER table points a manifest file (supplier.json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPjzu-yaXcRK"
      },
      "source": [
        "## Table Design and Query Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYe09ZVVXeht"
      },
      "source": [
        "You will analyze the affects of Compression, De-Normalization, Distribution and Sorting on Redshift query performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fb5lWN4X7VO"
      },
      "source": [
        "### Result Set Caching and Execution Plan Reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMeobfn4X_BP"
      },
      "source": [
        "Redshift enables a result set cache to speed up retrieval of data when it knows that the data in the underlying table has not changed. It can also re-use compiled query plans when only the predicate of the query has changed.\n",
        "\n",
        "Execute the following query and note the query execution time. Since this is the first execution of this query Redshift will need to compile the query as well as cache the result set.\n",
        "\n",
        "```sql\n",
        "SELECT c_mktsegment, o_orderpriority, sum(o_totalprice)\n",
        "FROM customer c\n",
        "JOIN orders o on c_custkey = o_custkey\n",
        "GROUP BY c_mktsegment, o_orderpriority;\n",
        "```\n",
        "\n",
        "Execute the same query a second time and note the query execution time. In the second execution redshift will leverage the result set cache and return immediately.\n",
        "\n",
        "```sql\n",
        "SELECT c_mktsegment, o_orderpriority, sum(o_totalprice)\n",
        "FROM customer c\n",
        "JOIN orders o on c_custkey = o_custkey\n",
        "GROUP BY c_mktsegment, o_orderpriority;\n",
        "```\n",
        "\n",
        "Update data in the table and run the query again. When data in an underlying table has changed Redshift will be aware of the change and invalidate the result set cache associated to the query. Note the execution time is not as fast as Step 2, but faster than Step 1 because while it couldn’t re-use the cache it could re-use the compiled plan.\n",
        "\n",
        "```sql\n",
        "UPDATE customer\n",
        "SET c_mktsegment = c_mktsegment\n",
        "WHERE c_mktsegment = 'MACHINERY';\n",
        "```\n",
        "\n",
        "```sql\n",
        "VACUUM DELETE ONLY customer;\n",
        "```\n",
        "\n",
        "```sql\n",
        "SELECT c_mktsegment, o_orderpriority, sum(o_totalprice)\n",
        "FROM customer c\n",
        "JOIN orders o on c_custkey = o_custkey\n",
        "GROUP BY c_mktsegment, o_orderpriority;\n",
        "```\n",
        "\n",
        "Execute a new query with a predicate and note the query execution time. Since this is the first execution of this query Redshift will need to compile the query as well as cache the result set.\n",
        "\n",
        "```sql\n",
        "SELECT c_mktsegment, count(1)\n",
        "FROM Customer c\n",
        "WHERE c_mktsegment = 'MACHINERY'\n",
        "GROUP BY c_mktsegment;\n",
        "```\n",
        "\n",
        "Execute the query with a slightly different predicate and note that the execution time is faster than the prior execution even though a very similar amount of data was scanned and aggregated. This behavior is due to the re-use of the compile cache because only the predicate has changed. This type of pattern is typical for BI reporting where the SQL pattern remains consistent with different users retrieving data associated to different predicates.\n",
        "\n",
        "```sql\n",
        "SELECT c_mktsegment, count(1)\n",
        "FROM customer c\n",
        "WHERE c_mktsegment = 'BUILDING'\n",
        "GROUP BY c_mktsegment;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1tY22zUCcG9"
      },
      "source": [
        "For the remainder of this lab turn off result set caching to ensure the runtimes are representative of an adhoc user query.\n",
        "Make sure to replace [Your-Redshift_User] value in the script below.\n",
        "\n",
        "```sql\n",
        "ALTER USER [Your-Redshift_User] set enable_result_cache_for_session to false;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URbZSMyUZ8R2"
      },
      "source": [
        "### Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aC2dMgRZ6qb"
      },
      "source": [
        "Redshift operates on high amounts of data. In order to optimize Redshift workloads, one of the key principles is to lower the amount of data stored. Instead of working on entire rows of data, containing values of different types and function, Redshift operates in a columnar fashion. This gives the opportunity to implement algorithms that can operate on single columns of data which can be compressed independently.\n",
        "\n",
        "The lineitem table was defined without any specified compression encodings. Instead, when the data was loaded, the encodings were automatically applied using the defaults because the COMPUPDATE PRESET clause was used in the COPY statement. Execute the following query to determine the compression used for the lineitem table.\n",
        "\n",
        "```sql\n",
        "SELECT tablename, \"column\", encoding\n",
        "FROM pg_table_def\n",
        "WHERE schemaname = 'public' AND tablename = 'lineitem'\n",
        "```\n",
        "\n",
        "Create a copy of lineitem table setting the ENCODING of each column to RAW and load that table with the lineitem data.\n",
        "\n",
        "```sql\n",
        "DROP TABLE IF EXISTS lineitem_v1;\n",
        "CREATE TABLE lineitem_v1 (\n",
        "  L_ORDERKEY bigint NOT NULL ENCODE RAW       ,\n",
        "  L_PARTKEY bigint ENCODE RAW                 ,\n",
        "  L_SUPPKEY bigint ENCODE RAW                 ,\n",
        "  L_LINENUMBER integer NOT NULL ENCODE RAW    ,\n",
        "  L_QUANTITY decimal(18,4) ENCODE RAW         ,\n",
        "  L_EXTENDEDPRICE decimal(18,4) ENCODE RAW    ,\n",
        "  L_DISCOUNT decimal(18,4) ENCODE RAW         ,\n",
        "  L_TAX decimal(18,4) ENCODE RAW              ,\n",
        "  L_RETURNFLAG varchar(1) ENCODE RAW          ,\n",
        "  L_LINESTATUS varchar(1) ENCODE RAW          ,\n",
        "  L_SHIPDATE date ENCODE RAW                  ,\n",
        "  L_COMMITDATE date ENCODE RAW                ,\n",
        "  L_RECEIPTDATE date ENCODE RAW               ,\n",
        "  L_SHIPINSTRUCT varchar(25) ENCODE RAW       ,\n",
        "  L_SHIPMODE varchar(10) ENCODE RAW           ,\n",
        "  L_COMMENT varchar(44) ENCODE RAW\n",
        ")\n",
        "distkey (L_ORDERKEY)\n",
        "sortkey (L_RECEIPTDATE);\n",
        "\n",
        "INSERT INTO lineitem_v1\n",
        "SELECT * FROM lineitem;\n",
        "\n",
        "ANALYZE lineitem_v1;\n",
        "```\n",
        "\n",
        "Redshift provides the ANALYZE COMPRESSION command. This command will determine the encoding for each column which will yield the most compression. Execute the ANALYZE COMPRESSION command on the table which was just loaded.\n",
        "\n",
        "```sql\n",
        "ANALYZE COMPRESSION lineitem_v1;\n",
        "```\n",
        "\n",
        "Note: While most columns have the same encodings, some columns will get better compression if the encoding is changed.\n",
        "\n",
        "Analyze the storage space for these tables, with and without compression. The table stores by column the amount of storage used in MB. You should see about a 70% savings on the storage of the second table compared to first. This query gives you the storage requirements per column for each table, then the total storage for the table (repeated identically on each line).\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  CAST(d.attname AS CHAR(50)),\n",
        "  SUM(CASE WHEN CAST(d.relname AS CHAR(50)) = 'lineitem'\n",
        "THEN b.size_in_mb ELSE 0 END) AS size_in_mb,\n",
        "  SUM(CASE WHEN CAST(d.relname AS CHAR(50)) = 'lineitem_v1'\n",
        "THEN b.size_in_mb ELSE 0 END) AS size_in_mb_v1,\n",
        "  SUM(SUM(CASE WHEN CAST(d.relname AS CHAR(50)) = 'lineitem'\n",
        "THEN b.size_in_mb ELSE 0 END)) OVER () AS total_mb,\n",
        "  SUM(SUM(CASE WHEN CAST(d.relname AS CHAR(50)) = 'lineitem_v1'\n",
        "THEN b.size_in_mb ELSE 0 END)) OVER () AS total_mb_v1\n",
        "FROM (\n",
        "  SELECT relname, attname, attnum - 1 as colid\n",
        "  FROM pg_class t\n",
        "  INNER JOIN pg_attribute a ON a.attrelid = t.oid\n",
        "  WHERE t.relname LIKE 'lineitem%') d\n",
        "INNER JOIN (\n",
        "  SELECT name, col, MAX(blocknum) AS size_in_mb\n",
        "  FROM stv_blocklist b\n",
        "  INNER JOIN stv_tbl_perm p ON b.tbl=p.id\n",
        "  GROUP BY name, col) b\n",
        "ON d.relname = b.name AND d.colid = b.col\n",
        "GROUP BY d.attname\n",
        "ORDER BY d.attname;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2SoMFuoifnF"
      },
      "source": [
        "## Modernize w/ Spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ6wCnT3iomq"
      },
      "source": [
        "In this lab, we show you how to query petabytes of data with Amazon Redshift and exabytes of data in your Amazon S3 data lake, without loading or moving objects. We will also demonstrate how you can leverage views which union data in direct attached storage as well as in your S3 Datalake to create a single source of truth. Finally, we will demonstrate strategies for aging off old data into S3 and maintaining only the most recent data in Amazon Redshift direct attached storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GB3phpIjC8h"
      },
      "source": [
        "### Build your DDL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGduMmfjGh3"
      },
      "source": [
        "Create a schema workshop_das and table workshop_das.green_201601_csv for tables that will reside on the Redshift compute nodes, AKA the Redshift direct-attached storage (DAS) tables.\n",
        "\n",
        "```sql\n",
        "CREATE SCHEMA workshop_das;\n",
        "\n",
        "CREATE TABLE workshop_das.green_201601_csv\n",
        "(\n",
        "  vendorid                VARCHAR(4),\n",
        "  pickup_datetime         TIMESTAMP,\n",
        "  dropoff_datetime        TIMESTAMP,\n",
        "  store_and_fwd_flag      VARCHAR(1),\n",
        "  ratecode                INT,\n",
        "  pickup_longitude        FLOAT4,\n",
        "  pickup_latitude         FLOAT4,\n",
        "  dropoff_longitude       FLOAT4,\n",
        "  dropoff_latitude        FLOAT4,\n",
        "  passenger_count         INT,\n",
        "  trip_distance           FLOAT4,\n",
        "  fare_amount             FLOAT4,\n",
        "  extra                   FLOAT4,\n",
        "  mta_tax                 FLOAT4,\n",
        "  tip_amount              FLOAT4,\n",
        "  tolls_amount            FLOAT4,\n",
        "  ehail_fee               FLOAT4,\n",
        "  improvement_surcharge   FLOAT4,\n",
        "  total_amount            FLOAT4,\n",
        "  payment_type            VARCHAR(4),\n",
        "  trip_type               VARCHAR(4)\n",
        ")\n",
        "DISTSTYLE EVEN\n",
        "SORTKEY (passenger_count,pickup_datetime);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0B6b-4IjRXe"
      },
      "source": [
        "### Build your Copy Command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_qBR2mvjQN6"
      },
      "source": [
        "Build your copy command to copy the data from Amazon S3. This dataset has the number of taxi rides in the month of January 2016.\n",
        "\n",
        "```sql\n",
        "COPY workshop_das.green_201601_csv\n",
        "FROM 's3://us-west-2.serverless-analytics/NYC-Pub/green/green_tripdata_2016-01.csv'\n",
        "IAM_ROLE default\n",
        "DATEFORMAT 'auto'\n",
        "IGNOREHEADER 1\n",
        "DELIMITER ','\n",
        "IGNOREBLANKLINES\n",
        "REGION 'us-west-2';\n",
        "```\n",
        "\n",
        "Determine how many rows you just loaded.\n",
        "\n",
        "```sql\n",
        "select count(1) from workshop_das.green_201601_csv;\n",
        "--1445285\n",
        "```\n",
        "\n",
        "Pin-point the Blizzard\n",
        "\n",
        "In this month, there is a date which had the lowest number of taxi rides due to a blizzard. Can you find that date?\n",
        "\n",
        "```sql\n",
        "SELECT TO_CHAR(pickup_datetime, 'YYYY-MM-DD'),\n",
        "COUNT(*)\n",
        "FROM workshop_das.green_201601_csv\n",
        "GROUP BY 1\n",
        "ORDER BY 2;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhwhLT5qjoEz"
      },
      "source": [
        "### Create external schema (and DB) for Redshift Spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMyrR_Xijy99"
      },
      "source": [
        "Because external tables are stored in a shared Glue Catalog for use within the AWS ecosystem, they can be built and maintained using a few different tools, e.g. Athena, Redshift, and Glue.\n",
        "\n",
        "Use the AWS Glue Crawler to create your external table adb305.month_12 stored in parquet format under location s3://serverless-analytics/canonical/NY-Pub/year=2016/month=12/.\n",
        "\n",
        "- Navigate to the Glue Crawler Page.  \n",
        "- Click on Add Crawler, and enter the crawler name NYTaxiCrawler and click Next. \n",
        "- Select Data stores as the source type and click Next. \n",
        "- Choose S3 as the data store and the include path of `s3://serverless-analytics/canonical/NY-Pub/year=2016/month=12/`.\n",
        "- Create an IAM Role and select it\n",
        "- Select Run on demand for the frequency. \n",
        "- Click on Add database and enter the Database of spectrumdb \n",
        "- Select all remaining defaults. Once the Crawler has been created, click on Run Crawler. \n",
        "- Once the Crawler has completed its run, you will see a new table in the Glue Catalog.\n",
        "- Note: For the next step, make sure to attach Glue access policy permissions into the Redshift role. Also, you need to add the glue service in the role's trust policy.\n",
        "- Now that the table has been cataloged, switch back to your Redshift query editor and create an external schema adb305 pointing to your Glue Catalog Database spectrumdb\n",
        "    ```sql\n",
        "    CREATE external SCHEMA adb305\n",
        "    FROM data catalog DATABASE 'spectrumdb'\n",
        "    IAM_ROLE default\n",
        "    CREATE external DATABASE if not exists;\n",
        "    ```\n",
        "- Run the query from the previous step using the external table instead of the direct-attached storage (DAS).\n",
        "    ```sql\n",
        "    SELECT TO_CHAR(pickup_datetime, 'YYYY-MM-DD'),\n",
        "    COUNT(*)\n",
        "    FROM adb305.month_12\n",
        "    GROUP BY 1\n",
        "    ORDER BY 1;\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVfVg1miLgU"
      },
      "source": [
        "### Plan for the Future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEzoNQoGiJKD"
      },
      "source": [
        "In this final part of this lab, we will compare different strategies for maintaining more recent or HOT data within Redshift direct-attached storage, and keeping older COLD data in S3 by performing the following steps:\n",
        "\n",
        "- Allow for trailing 5 quarters reporting by adding the Q4 2015 data to Redshift DAS:\n",
        "    - Anticipating that we’ll want to ”age-off” the oldest quarter on a 3 month basis, architect your DAS table to make this easy to maintain and query.\n",
        "    - Adjust your Redshift Spectrum table to exclude the Q4 2015 data.\n",
        "- Develop and execute a plan to move the Q4 2015 data to S3.\n",
        "    - What are the discrete steps to be performed?\n",
        "    - What extra-Redshift functionality must be leveraged?\n",
        "    - Simulating the extra-Redshift steps with the existing Parquet data, age-off the Q4 2015 data from Redshift DAS and perform any needed steps to maintain a single version of the truth.\n",
        "- There are several options to accomplish this goal. Anticipating that we’ll want to ”age-off” the oldest quarter on a 3 month basis, architect your DAS table to make this easy to maintain and query. How about something like this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvpdpl7UmJEI"
      },
      "source": [
        "## Spectrum Query Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTo6ofhfmLPp"
      },
      "source": [
        "In this lab, we show you how to diagnose your Redshift Spectrum query performance and optimize performance by leveraging partitions, optimizing storage, and predicate pushdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axKduTPdme6K"
      },
      "source": [
        "### Querying with Amazon Redshift Spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ysQW5NLmhHT"
      },
      "source": [
        "Create a star schema data model by creating dimension tables in your Redshift cluster, and fact tables in S3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4e9BL5zmkhr"
      },
      "source": [
        "1. Create the dimension tables by running this script from your client tool.\n",
        "    ```sql\n",
        "    DROP TABLE IF EXISTS customer;\n",
        "    CREATE TABLE customer (\n",
        "    c_custkey     \tinteger        not null sortkey,\n",
        "    c_name        \tvarchar(25)    not null,\n",
        "    c_address     \tvarchar(25)    not null,\n",
        "    c_city        \tvarchar(10)    not null,\n",
        "    c_nation      \tvarchar(15)    not null,\n",
        "    c_region      \tvarchar(12)    not null,\n",
        "    c_phone       \tvarchar(15)    not null,\n",
        "    c_mktsegment      varchar(10)    not null)\n",
        "    diststyle all;\n",
        "\n",
        "    DROP TABLE IF EXISTS dwdate;\n",
        "    CREATE TABLE dwdate (\n",
        "    d_datekey            integer       not null sortkey,\n",
        "    d_date               varchar(19)   not null,\n",
        "    d_dayofweek\t      varchar(10)   not null,\n",
        "    d_month      \t    varchar(10)   not null,\n",
        "    d_year               integer       not null,\n",
        "    d_yearmonthnum       integer  \t not null,\n",
        "    d_yearmonth          varchar(8)\tnot null,\n",
        "    d_daynuminweek       integer       not null,\n",
        "    d_daynuminmonth      integer       not null,\n",
        "    d_daynuminyear       integer       not null,\n",
        "    d_monthnuminyear     integer       not null,\n",
        "    d_weeknuminyear      integer       not null,\n",
        "    d_sellingseason      varchar(13)    not null,\n",
        "    d_lastdayinweekfl    varchar(1)    not null,\n",
        "    d_lastdayinmonthfl   varchar(1)    not null,\n",
        "    d_holidayfl          varchar(1)    not null,\n",
        "    d_weekdayfl          varchar(1)    not null)\n",
        "    diststyle all;\n",
        "    ```\n",
        "1. Load data into your dimension table by running the following script. You will need to provide an IAM role with the permissions to run the COPY command on your cluster. You can use the IAM role that you identified earlier. This will load the data set from S3 into your Redshift cluster. Expect the script to take a few minutes to complete. The customer and time dimension consists of 3M records, and 2556 records respectively.\n",
        "    ```sql\n",
        "    copy customer from 's3://awssampledbuswest2/ssbgz/customer'\n",
        "    iam_role default\n",
        "    gzip region 'us-west-2';\n",
        "\n",
        "    copy dwdate from 's3://awssampledbuswest2/ssbgz/dwdate'\n",
        "    iam_role default\n",
        "    gzip region 'us-west-2';\n",
        "    ```\n",
        "1. Next, create an External Schema that references datasets that reside outside of your Redshift cluster. Define this schema by running the following command. You will need to provide an IAM role with the permissions to read S3 date from your cluster. This should be the same role used above in the COPY command. Redshift stores the meta-data that describes your external databases and schemas in the AWS Glue data catalog by default. Once created, you can view the schema from Glue or Athena.\n",
        "    ```sql\n",
        "    CREATE EXTERNAL SCHEMA clickstream\n",
        "    from data catalog database 'clickstream'\n",
        "    iam_role default\n",
        "    CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
        "    ```\n",
        "1. Use the AWS Glue Crawler to create your external table clickstream.clickstream-csv10 and clickstream.clickstream-parquet1 under locations s3://wysde-datasets/clickstream.\n",
        "    - Navigate to the Glue Crawler Page\n",
        "    - Add Crawler, and enter the crawler name clickstream and click Next\n",
        "    - Choose S3 as the data store and the include path of s3://wysde-datasets/clickstream\n",
        "    - Click on Add database. Name it clickstream and click Create. Select the Database clickstream from the list\n",
        "    - Select all remaining defaults. Once the Crawler has been created, click on Run Crawler.\n",
        "    - Once the Crawler has completed its run, you will see two new tables in the Glue Catalog.\n",
        "    - For uservisits_csv10 table, we need to correct the schema. Click on Edit Schema and adjust the column names and datatypes as per the table below. Click Save.\n",
        "    ```\n",
        "    | #  | Column name    | Data type | Partition key | Comment |\n",
        "    | -- | -------------- | --------- | ------------- | ------- |\n",
        "    | 1  | adrevenue      | double    | \\-            | \\-      |\n",
        "    | 2  | countrycode    | string    | \\-            | \\-      |\n",
        "    | 3  | custkey        | bigint    | \\-            | \\-      |\n",
        "    | 4  | desturl        | string    | \\-            | \\-      |\n",
        "    | 5  | duration       | bigint    | \\-            | \\-      |\n",
        "    | 6  | languagecode   | string    | \\-            | \\-      |\n",
        "    | 7  | searchword     | string    | \\-            | \\-      |\n",
        "    | 8  | sourceip       | string    | \\-            | \\-      |\n",
        "    | 9  | useragent      | string    | \\-            | \\-      |\n",
        "    | 10 | visitdate      | bigint    | \\-            | \\-      |\n",
        "    | 11 | yearmonthkey   | bigint    | \\-            | \\-      |\n",
        "    | 12 | customer       | string    | Partition (0) | \\-      |\n",
        "    | 13 | visityearmonth | string    | Partition (1) | \\-      |\n",
        "    ```\n",
        "1. Navigate back to your SQL Client tool and run the query below. This query performs a join between dimension tables in Redshift, and the clickstream fact table in S3 effectively blending data from the data Lake and data warehouse. The ad revenue data originates from S3 while the customer and time attributes like market segment originate from the dimension tables in Redshift.\n",
        "    ```sql\n",
        "    SELECT c.c_name, c.c_mktsegment, t.prettyMonthYear, SUM(uv.adRevenue)\n",
        "    FROM clickstream.uservisits_csv10 as uv\n",
        "    RIGHT OUTER JOIN customer as c ON c.c_custkey = uv.custKey\n",
        "    INNER JOIN (\n",
        "    SELECT DISTINCT d_yearmonthnum, (d_month||','||d_year) as prettyMonthYear\n",
        "    FROM dwdate\n",
        "    WHERE d_yearmonthnum >= 199410) as t ON uv.yearMonthKey = t.d_yearmonthnum\n",
        "    WHERE c.c_custkey <= 5\n",
        "    GROUP BY c.c_name, c.c_mktsegment, t.prettyMonthYear, uv.yearMonthKey\n",
        "    ORDER BY c.c_name, c.c_mktsegment, uv.yearMonthKey  ASC\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8T3tNWgyqN2"
      },
      "source": [
        "### Performance Diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JHHtG8xyobf"
      },
      "source": [
        "There are a few utilities that provide visibility into Redshift Spectrum:\n",
        "\n",
        "- EXPLAIN  - Provides the query execution plan, which includes info around what processing is pushed down to Spectrum. Steps in the plan that include the prefix S3 are executed on Spectrum; for instance, the plan for the query above has a step “S3 Seq Scan clickstream.uservisits_csv10” indicating that Spectrum performs a scan on S3 as part of the query execution.\n",
        "- SVL_S3QUERY_SUMMARY  - Provides statistics for Redshift Spectrum queries are stored in this table. While the execution plan presents cost estimates, this table stores actual statistics of past query runs.\n",
        "- SVL_S3PARTITION  - Provides details about Amazon Redshift Spectrum partition pruning at the segment and node slice level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SncXdKCp3uS-"
      },
      "source": [
        "## ETL/ELT Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZY9ZBW83xKw"
      },
      "source": [
        "This lab demonstrates how you can modernize your ETL/ELT processes using Materialized Views, Stored Procedures, and Query Scheduling to transform data within Redshift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya92hIO6320S"
      },
      "source": [
        "### Materialized Views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI2piekE305r"
      },
      "source": [
        "In a data warehouse environment, applications often need to perform complex queries on large tables—for example, SELECT statements that perform multi-table joins and aggregations on the tables that contain billions of rows. Processing these queries can be expensive in terms of system resources and the time it takes to compute the results. Materialized views in Amazon Redshift provide a way to address these issues. A materialized view contains a precomputed result set, based on SQL query over one or more base tables. Here you will learn how to create, query and refresh a materialized view.\n",
        "\n",
        "Let’s take an example where you want to generate a report of the top suppliers by shipped quantity. This will join large tables like and lineitem, and suppliers and scan a large quantity of data. You might write a query like the following:\n",
        "\n",
        "```sql\n",
        "select n_name, s_name, l_shipmode,\n",
        "  SUM(L_QUANTITY) Total_Qty\n",
        "from lineitem\n",
        "join supplier on l_suppkey = s_suppkey\n",
        "join nation on s_nationkey = n_nationkey\n",
        "where datepart(year, L_SHIPDATE) > 1997\n",
        "group by 1,2,3\n",
        "order by 3 desc\n",
        "limit 1000;\n",
        "```\n",
        "\n",
        "This query takes time to execute and because it is scanning a large amount of data will use a lot of I/O & CPU resources. Think of a situation, where multiple users in the organization need get supplier-level metrics like the above. Each may write similarly heavy queries which can be time consuming and expensive operations. Instead of that you can use a materialized view to store precomputed results for speeding up queries that are predictable and repeated.\n",
        "\n",
        "Amazon Redshift provides a few methods to keep materialized views up-to-date. You can configure the automatic refresh option to refresh materialized views when base tables of mare updated. The auto refresh operation runs at a time when cluster resources are available to minimize disruptions to other workloads.\n",
        "\n",
        "Execute below query to create materialized view which aggregates the lineitem data to the supplier level. Note, the AUTO REFRESH option is set to YES and we've included additional columns in our MV in case other users can take advantage of this aggregated data.\n",
        "\n",
        "```sql\n",
        "CREATE MATERIALIZED VIEW supplier_shipmode_agg\n",
        "AUTO REFRESH YES AS\n",
        "select l_suppkey, l_shipmode, datepart(year, L_SHIPDATE) l_shipyear,\n",
        "  SUM(L_QUANTITY)\tTOTAL_QTY,\n",
        "  SUM(L_DISCOUNT) TOTAL_DISCOUNT,\n",
        "  SUM(L_TAX) TOTAL_TAX,\n",
        "  SUM(L_EXTENDEDPRICE) TOTAL_EXTENDEDPRICE  \n",
        "from LINEITEM\n",
        "group by 1,2,3;\n",
        "```\n",
        "\n",
        "Now execute the below query which has been re-written to use the materialized view. Note the difference in query execution time. You get the same results in few seconds.\n",
        "\n",
        "```sql\n",
        "select n_name, s_name, l_shipmode,\n",
        "  SUM(TOTAL_QTY) Total_Qty\n",
        "from supplier_shipmode_agg\n",
        "join supplier on l_suppkey = s_suppkey\n",
        "join nation on s_nationkey = n_nationkey\n",
        "where l_shipyear > 1997\n",
        "group by 1,2,3\n",
        "order by 3 desc\n",
        "limit 1000;\n",
        "```\n",
        "\n",
        "Another powerful feature of Materialized view is auto query rewrite. Amazon Redshift can automatically rewrite queries to use materialized views, even when the query doesn't explicitly reference a materialized view.\n",
        "\n",
        "Now, re-run your original query which references the lineitem table and see this query now executes faster because Redshift has re-written this query to leverage the materialized view instead of base table.\n",
        "\n",
        "```sql\n",
        "select n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty\n",
        "from lineitem\n",
        "join supplier on l_suppkey = s_suppkey\n",
        "join nation on s_nationkey = n_nationkey\n",
        "where datepart(year, L_SHIPDATE) > 1997\n",
        "group by 1,2,3\n",
        "order by 3 desc\n",
        "limit 1000;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWS2BFb44Ux6"
      },
      "source": [
        "### Stored procedures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgJ_eaBp4sKt"
      },
      "source": [
        "Stored procedures are commonly used to encapsulate logic for data transformation, data validation, and business-specific logic. By combining multiple SQL steps into a stored procedure, you can reduce round trips between your applications and the database. A stored procedure can incorporate data definition language (DDL) and data manipulation language (DML) in addition to SELECT queries. A stored procedure doesn’t have to return a value. You can use the PL/pgSQL procedural language, including looping and conditional expressions, to control logical flow.\n",
        "\n",
        "Let’s see how you can create and invoke stored procedure in Redshift. Here our goal is to incrementally refresh the lineitem data. Execute the following query to create lineitem staging table:\n",
        "\n",
        "```sql\n",
        "create table stage_lineitem (\n",
        "  L_ORDERKEY bigint NOT NULL,\n",
        "  L_PARTKEY bigint,\n",
        "  L_SUPPKEY bigint,\n",
        "  L_LINENUMBER integer NOT NULL,\n",
        "  L_QUANTITY decimal(18,4),\n",
        "  L_EXTENDEDPRICE decimal(18,4),\n",
        "  L_DISCOUNT decimal(18,4),\n",
        "  L_TAX decimal(18,4),\n",
        "  L_RETURNFLAG varchar(1),\n",
        "  L_LINESTATUS varchar(1),\n",
        "  L_SHIPDATE date,\n",
        "  L_COMMITDATE date,\n",
        "  L_RECEIPTDATE date,\n",
        "  L_SHIPINSTRUCT varchar(25),\n",
        "  L_SHIPMODE varchar(10),\n",
        "  L_COMMENT varchar(44));\n",
        "```\n",
        "\n",
        "Execute below script to create a stored procedure. This stored procedure performs following tasks:\n",
        "- Truncate staging table to clean up old data\n",
        "- Load data in the stage_lineitem table using the COPY command.\n",
        "- Merge updated records in existing lineitem table.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE PROCEDURE lineitem_incremental()\n",
        "AS $$\n",
        "BEGIN\n",
        "\n",
        "truncate stage_lineitem;  \n",
        "\n",
        "copy stage_lineitem from 's3://redshift-immersionday-labs/data/lineitem-part/l_orderyear=1998/l_ordermonth=8/'\n",
        "iam_role default\n",
        "region 'us-west-2' gzip delimiter '|' COMPUPDATE PRESET;\n",
        "\n",
        "delete from lineitem using stage_lineitem\n",
        "where stage_lineitem.l_orderkey=lineitem.l_orderkey and stage_lineitem.l_linenumber = lineitem.l_linenumber;\n",
        "\n",
        "insert into lineitem\n",
        "select * from stage_lineitem;\n",
        "\n",
        "END;\n",
        "$$ LANGUAGE plpgsql;\n",
        "```\n",
        "\n",
        "Before you call this stored procedure, capture a metric using the materialized view. We'll compare this value after the stored procedure loads new data to demonstrate the Materialized View auto refresh capability.\n",
        "\n",
        "```sql\n",
        "select SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg;\n",
        "```\n",
        "\n",
        "Call this stored procedure using CALL statement. When executed it will perform an incremental load:\n",
        "\n",
        "```sql\n",
        "call lineitem_incremental();\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tju666zG5jyu"
      },
      "source": [
        "### Query scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmJHhcOO5ig5"
      },
      "source": [
        "Amazon Redshift allows you to schedule your SQL queries for executions in recurring schedules. You can now schedule time sensitive or long running queries, loading or unloading your data, stored procedures or refreshing your materialized views on a regular schedule. You can use the Amazon Redshift Console or Amazon Redshift Data API to schedule your SQL queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcIThqzj5qPg"
      },
      "source": [
        "1. Navigate, back to Redshift query editor, ensure the query to call your stored procedure is in the editor and click on the Schedule button.\n",
        "1. Select IAM Role, select the cluster, and provide a database name and database user.\n",
        "1. Enter a query name as well as the query text.\n",
        "1. Provide values for Repeat By, Repeat every, and Repeat time. When you select “Repeat at time (UTC)” enter a time that is little later than current time so you can observe the execution. Optionally, you can enable monitoring via Amazon SNS notifications. For this example, we can leave this Disabled.\n",
        "1. Navigate to the scheduled queries tab and you can see your query scheduler has been created.\n",
        "1. Click on the schedule and after successful execution on scheduled time, you can see the status is “success”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- https://catalog.us-east-1.prod.workshops.aws/workshops/9f29cdba-66c0-445e-8cbb-28a092cb5ba7/en-US"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \n[Clang 6.0 (clang-600.0.57)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
