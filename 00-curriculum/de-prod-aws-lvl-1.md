# Curriculum - Data Engineering Training (AWS)

**Estimated Time:** 55 hours

## Modules

* [ ] Developer Foundations
* [ ] Data Engineering Foundations
* [ ] AWS Cloud Essentials
* [ ] Programming - SQL
* [ ] Programming - Python
* [ ] Programming - PySpark
* [ ] Programming - Scala
* [ ] Data Modeling and Warehousing
* [ ] Data Lakes and Lakehouses
* [ ] Big Data Processing
* [ ] Orchestration and Data Pipelines
* [ ] Capstone Projects

## Developer Foundations

* [ ] Download and Install vscode
* [ ] Understand vscode features
* [ ] Install extensions in vscode
* [ ] Download and Install Anaconda
* [ ] Create virtual environment in anaconda
* [ ] Create jupyter notebook in vscode and connect to venv
* [ ] Create github account
* [ ] Install git cli
* [ ] Create git repo and add students as collaborator
* [ ] Connect local workspace to git repo
* [ ] Learn git commands
* [ ] Learn bash commands
* [ ] Download and Install DBeaver

## Data Engineering Foundations

* [ ] What is Data Engineering?
* [ ] Role of Data Engineering in Organizations
* [ ] Skills required to become a Data Engineer
* [ ] Data engineer versus data scientist
* [ ] What is data lake and data warehouse?
* [ ] What is medallion architecture?
* [ ] What is EL, ETL and ELT?

## AWS Cloud Essentials

* [ ] (Optional) Create AWS Account
* [ ] Create IAM user and generate credentials
* [ ] Install AWS CLI
* [ ] Setup AWS credentials
* [ ] Walkthrough of various AWS Services (S3, RDS, Redshift, Glue, Athena, Lambda, EMR, Keyspace, Cloudformation, IAM, Secrets Manager)
* [ ] Comparison of AWS services with GCP Services (GCS Bucket, Dataproc, IAM, Cloud Functions, BigQuery, CloudSQL, BigTable)
* [ ] Comparison of AWS services with Azure Services (Blob Storge, DataLake Gen2 buckets, Databricks/Synapse Analytics, Azure Data Factory, Azure SQL Databases)
* [ ] Learn AWS CLI S3 essential commands
* [ ] Copy and Sync data to/from S3 with AWS CLI
* [ ] Create database in RDS DBMS and generate credentials
* [ ] Connect to RDS DBMS in DBeaver
* [ ] Create a Secret in Secrets Manager Vault
* [ ] Get the credential using AWS CLI

## Programming - SQL

* [ ] Ingest data from CSV file into MySQL database table
* [ ] SQL Basics - Select, Limit, Where, Comparison and Logical operators, Order by
* [ ] SQL Intermediate - Aggregations, Group by, Case statements, Joins
* [ ] SQL Advanced - Dates, Text, Subqueries, Window functions, Optimizations

## Programming - Python

* [ ] Lists and dictionaries
* [ ] For loops and while loops
* [ ] Functions and Inline functions
* [ ] Read/Write and Manipulate Data using Pandas
* [ ] Pulling data from APIs using requests library
* [ ] Reading and writing data to databases using psycopg2 and sqlalchemy library
* [ ] Reading data from S3 and athena using aws data wrangler library
* [ ] Pull credentials from Secrets Manager using boto3 library
* [ ] Lab - Exchange Rate ETL process [[source code](../01-foundations/language/python/lab-exchange-rate-etl)]

## Programming - PySpark

* [ ] Create databricks account
* [ ] Create your first databricks cluster
* [ ] Create your first databricks notebook
* [ ] M&M color balls analysis with PySpark
* [ ] Movielens and Song analysis with PySpark
* [ ] San Francisco Fire Department call analysis with PySpark
* [ ] Connect AWS to PySpark and build an ETL pipeline

## Programming - Scala

* [ ] Introduction to Scala programming
* [ ] Getting started with Spark Scala
* [ ] Building extract and load pipeline with Scala, S3 and Postgres

## Data Modeling and Warehousing

* [ ] Building a sql data model for a music company in Postgres
* [ ] Building a nosql data model for a music company in Cassandra
* [ ] Difference between databases, warehouses, lakes and lakehouses
* [ ] OLTP vs OLAP technologies
* [ ] Loading data into Redshift warehouse with S3 staging and COPY command

## Data Lakes and Lakehouses

* [ ] Building a data lake for a healthcare company with AWS, S3 and Athena
* [ ] Working with AWS S3 and Delta lake in Databricks

## Big Data Processing

* [ ] Creating and Monitoring Production Data Processing Jobs in Databricks
* [ ] Creating and submitting Word count Spark Job in EMR Serverless
* [ ] Building a near real-time serverless data pipeline with AWS lambda function
* [ ] Building an ELT pipeline for a cab service company using dbt and Postgres

## Orchestration and Data Pipelines

* [ ] Install Airflow in your PC
* [ ] Starting Airflow scheduler and web server
* [ ] Building a BASH commands execution pipeline in Airflow
* [ ] Building a CSV to JSON pipeline in Airflow
* [ ] Integrate email notifications in Airflow with AWS SNS/SES service

## Capstone Projects

* [ ] ACLED ETL Data Pipeline for war and conflict analysis (Airflow, Postgres, Glue, Spark)
* [ ] Sales & Orders ELT Data Pipeline (dbt, Redshift, SQL, Jinja)
